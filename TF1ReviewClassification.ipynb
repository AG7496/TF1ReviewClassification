{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54BRvd5m6vDb"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwOfP0DxNEwv",
        "outputId": "ddd64791-d643-4b8b-ef1f-0168ae10d289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/raw_data\n",
            "--2022-06-06 21:27:53--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 495854086 (473M) [application/x-gzip]\n",
            "Saving to: ‘reviews_Electronics_5.json.gz’\n",
            "\n",
            "reviews_Electronics 100%[===================>] 472.88M  37.3MB/s    in 14s     \n",
            "\n",
            "2022-06-06 21:28:08 (33.4 MB/s) - ‘reviews_Electronics_5.json.gz’ saved [495854086/495854086]\n",
            "\n",
            "--2022-06-06 21:28:19--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 186594679 (178M) [application/x-gzip]\n",
            "Saving to: ‘meta_Electronics.json.gz’\n",
            "\n",
            "meta_Electronics.js 100%[===================>] 177.95M  32.3MB/s    in 6.5s    \n",
            "\n",
            "2022-06-06 21:28:26 (27.3 MB/s) - ‘meta_Electronics.json.gz’ saved [186594679/186594679]\n",
            "\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!mkdir raw_data/\n",
        "%cd raw_data\n",
        "!wget -c http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\n",
        "!gzip -d reviews_Electronics_5.json.gz\n",
        "!wget -c http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz\n",
        "!gzip -d meta_Electronics.json.gz\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5M9MddP6yyk"
      },
      "source": [
        "# Convert tensorflow version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmPfB1ny6fDy",
        "outputId": "34e0c433-ebd4-4b92-b0ec-120211a45f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.13.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOh21Fjd7eXe"
      },
      "source": [
        "# Convert dataset to pandas version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6aXPjn85mId"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def to_df(file_path):\n",
        "  with open(file_path, 'r') as fin:\n",
        "    df = {}\n",
        "    i = 0\n",
        "    for line in fin:\n",
        "      df[i] = eval(line)\n",
        "      i += 1\n",
        "    df = pd.DataFrame.from_dict(df, orient='index')\n",
        "    return df\n",
        "\n",
        "reviews_df = to_df('raw_data/reviews_Electronics_5.json')\n",
        "with open('raw_data/reviews.pkl', 'wb') as f:\n",
        "  pickle.dump(reviews_df, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "meta_df = to_df('raw_data/meta_Electronics.json')\n",
        "meta_df = meta_df[meta_df['asin'].isin(reviews_df['asin'].unique())]\n",
        "meta_df = meta_df.reset_index(drop=True)\n",
        "with open('raw_data/meta.pkl', 'wb') as f:\n",
        "  pickle.dump(meta_df, f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdua3GbO7iMO"
      },
      "source": [
        "# Remap dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn_npx6A6J1w",
        "outputId": "be178817-d49f-47cb-a7c4-1473f9bacdff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_count: 192403\titem_count: 63001\tcate_count: 801\texample_count: 1689188\n"
          ]
        }
      ],
      "source": [
        "with open('raw_data/reviews.pkl', 'rb') as f:\n",
        "  reviews_df = pickle.load(f)\n",
        "  reviews_df = reviews_df[['reviewerID', 'asin', 'unixReviewTime']]\n",
        "\n",
        "with open('raw_data/meta.pkl', 'rb') as f:\n",
        "  meta_df = pickle.load(f)\n",
        "  meta_df = meta_df[['asin', 'categories']]\n",
        "  meta_df['categories'] = meta_df['categories'].map(lambda x: x[-1][-1])\n",
        "\n",
        "\n",
        "def build_map(df, col_name):\n",
        "  key = sorted(df[col_name].unique().tolist())\n",
        "  m = dict(zip(key, range(len(key))))\n",
        "  df[col_name] = df[col_name].map(lambda x: m[x])\n",
        "  return m, key\n",
        "\n",
        "asin_map, asin_key = build_map(meta_df, 'asin')\n",
        "cate_map, cate_key = build_map(meta_df, 'categories')\n",
        "revi_map, revi_key = build_map(reviews_df, 'reviewerID')\n",
        "\n",
        "user_count, item_count, cate_count, example_count =\\\n",
        "    len(revi_map), len(asin_map), len(cate_map), reviews_df.shape[0]\n",
        "print('user_count: %d\\titem_count: %d\\tcate_count: %d\\texample_count: %d' %\n",
        "      (user_count, item_count, cate_count, example_count))\n",
        "\n",
        "meta_df = meta_df.sort_values('asin')\n",
        "meta_df = meta_df.reset_index(drop=True)\n",
        "reviews_df['asin'] = reviews_df['asin'].map(lambda x: asin_map[x])\n",
        "reviews_df = reviews_df.sort_values(['reviewerID', 'unixReviewTime'])\n",
        "reviews_df = reviews_df.reset_index(drop=True)\n",
        "reviews_df = reviews_df[['reviewerID', 'asin', 'unixReviewTime']]\n",
        "\n",
        "cate_list = [meta_df['categories'][i] for i in range(len(asin_map))]\n",
        "cate_list = np.array(cate_list, dtype=np.int32)\n",
        "\n",
        "\n",
        "with open('raw_data/remap.pkl', 'wb') as f:\n",
        "  pickle.dump(reviews_df, f, pickle.HIGHEST_PROTOCOL) \n",
        "  pickle.dump(cate_list, f, pickle.HIGHEST_PROTOCOL) \n",
        "  pickle.dump((user_count, item_count, cate_count, example_count),\n",
        "              f, pickle.HIGHEST_PROTOCOL)\n",
        "  pickle.dump((asin_key, cate_key, revi_key), f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sLVjgqP7rYO"
      },
      "source": [
        "# Build dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJgHhJwMR8zA"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "random.seed(1234)\n",
        "\n",
        "with open('raw_data/remap.pkl', 'rb') as f:\n",
        "  reviews_df = pickle.load(f)\n",
        "  cate_list = pickle.load(f)\n",
        "  user_count, item_count, cate_count, example_count = pickle.load(f)\n",
        "\n",
        "train_set = []\n",
        "test_set = []\n",
        "for reviewerID, hist in reviews_df.groupby('reviewerID'):\n",
        "  pos_list = hist['asin'].tolist()\n",
        "  def gen_neg():\n",
        "    neg = pos_list[0]\n",
        "    while neg in pos_list:\n",
        "      neg = random.randint(0, item_count-1)\n",
        "    return neg\n",
        "  neg_list = [gen_neg() for i in range(len(pos_list))]\n",
        "  rid_list = [reviewerID for i in range(len(pos_list))]\n",
        "  hist = list(zip(rid_list, pos_list, neg_list))\n",
        "\n",
        "  train_set.extend(hist[:-1])\n",
        "  test_set.append(hist[-1])\n",
        "\n",
        "random.shuffle(train_set)\n",
        "random.shuffle(test_set)\n",
        "\n",
        "assert len(test_set) == user_count\n",
        "assert len(test_set) + len(train_set) == example_count\n",
        "\n",
        "train_set = np.array(train_set, dtype=np.int32)\n",
        "test_set = np.array(test_set, dtype=np.int32)\n",
        "\n",
        "\n",
        "with open('dataset.pkl', 'wb') as f:\n",
        "  pickle.dump(train_set, f, pickle.HIGHEST_PROTOCOL)\n",
        "  pickle.dump(test_set, f, pickle.HIGHEST_PROTOCOL)\n",
        "  pickle.dump(cate_list, f, pickle.HIGHEST_PROTOCOL)\n",
        "  pickle.dump((user_count, item_count, cate_count), f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRANuJ7394OD"
      },
      "source": [
        "# Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhsNB2Hf958v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Model(object):\n",
        "\n",
        "  def __init__(self, user_count, item_count, cate_count, cate_list):\n",
        "    self.u = tf.placeholder(tf.int32, [None,])\n",
        "    self.i = tf.placeholder(tf.int32, [None,])\n",
        "    self.j = tf.placeholder(tf.int32, [None,])\n",
        "    self.lr = tf.placeholder(tf.float64, [])\n",
        "\n",
        "    user_emb_w = tf.get_variable(\"user_emb_w\", [user_count, 128])\n",
        "    item_emb_w = tf.get_variable(\"item_emb_w\", [item_count, 64])\n",
        "    item_b = tf.get_variable(\"item_b\", [item_count])\n",
        "    cate_emb_w = tf.get_variable(\"cate_emb_w\", [cate_count, 64])\n",
        "    cate_list = tf.convert_to_tensor(cate_list, dtype=tf.int64)\n",
        "\n",
        "    u_emb = tf.nn.embedding_lookup(user_emb_w, self.u)\n",
        "\n",
        "    ic = tf.gather(cate_list, self.i)\n",
        "    i_emb = tf.concat([\n",
        "        tf.nn.embedding_lookup(item_emb_w, self.i),\n",
        "        tf.nn.embedding_lookup(cate_emb_w, ic),\n",
        "        ], 1)\n",
        "    i_b = tf.gather(item_b, self.i)\n",
        "\n",
        "    jc = tf.gather(cate_list, self.j)\n",
        "    j_emb = tf.concat([\n",
        "        tf.nn.embedding_lookup(item_emb_w, self.j),\n",
        "        tf.nn.embedding_lookup(cate_emb_w, jc),\n",
        "        ], 1)\n",
        "    j_b = tf.gather(item_b, self.j)\n",
        "\n",
        "    # MF predict: u_i > u_j\n",
        "    x = i_b - j_b + tf.reduce_sum(tf.multiply(u_emb, (i_emb - j_emb)), 1)\n",
        "    self.logits = tf.sigmoid(x)\n",
        "\n",
        "    # AUC for one user:\n",
        "    # reasonable iff all (u,i,j) pairs are from the same user\n",
        "    # average AUC = mean( auc for each user in test set)\n",
        "    self.mf_auc = tf.reduce_mean(tf.to_float(x > 0))\n",
        "\n",
        "    # logits for all item:\n",
        "    all_emb = tf.concat([\n",
        "        item_emb_w,\n",
        "        tf.nn.embedding_lookup(cate_emb_w, cate_list)\n",
        "        ], axis=1)\n",
        "    self.logits_all = tf.sigmoid(\n",
        "        item_b + tf.matmul(u_emb, all_emb, transpose_b=True))\n",
        "\n",
        "    l2_norm = tf.add_n([\n",
        "        tf.nn.l2_loss(u_emb),\n",
        "        tf.nn.l2_loss(i_emb),\n",
        "        tf.nn.l2_loss(j_emb),\n",
        "        ])\n",
        "\n",
        "    reg_rate = 5e-5\n",
        "    self.bprloss = reg_rate * l2_norm - tf.reduce_mean(tf.log(self.logits))\n",
        "\n",
        "    opt = tf.train.GradientDescentOptimizer\n",
        "    self.train_op = opt(self.lr).minimize(self.bprloss)\n",
        "\n",
        "  def train(self, sess, uij, l):\n",
        "    loss, _ = sess.run([self.bprloss, self.train_op], feed_dict={\n",
        "        self.u: uij[:, 0],\n",
        "        self.i: uij[:, 1],\n",
        "        self.j: uij[:, 2],\n",
        "        self.lr: l,\n",
        "        })\n",
        "    return loss\n",
        "\n",
        "  def eval(self, sess, test_set):\n",
        "    return sess.run(self.mf_auc, feed_dict={\n",
        "        self.u: test_set[:, 0],\n",
        "        self.i: test_set[:, 1],\n",
        "        self.j: test_set[:, 2],\n",
        "        })\n",
        "\n",
        "  def test(self, sess, uid):\n",
        "    return sess.run(self.logits_all, feed_dict={\n",
        "        self.u: uid,\n",
        "        })\n",
        "\n",
        "  def save(self, sess, path):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_path=path)\n",
        "\n",
        "  def restore(self, sess, path):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, save_path=path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJdeLP1--BTR"
      },
      "source": [
        "# Data input "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY7ZAMOW-DQN"
      },
      "outputs": [],
      "source": [
        "class DataInput:\n",
        "  def __init__(self, data, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.data = data\n",
        "    self.epoch_size = self.data.shape[0] // self.batch_size\n",
        "    if self.epoch_size * self.batch_size < self.data.shape[0]:\n",
        "      self.epoch_size += 1\n",
        "    self.i = 0\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  def __next__(self):\n",
        "    if self.i == self.epoch_size:\n",
        "      raise StopIteration\n",
        "\n",
        "    t = self.data[self.i * self.batch_size : min((self.i+1) * self.batch_size,\n",
        "                                                 self.data.shape[0])]\n",
        "    self.i += 1\n",
        "\n",
        "    return self.i, t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPLQHeh37wBr"
      },
      "source": [
        "# Train and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTkfLcgwSh6d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "train_batch_size = 32\n",
        "\n",
        "with open('dataset.pkl', 'rb') as f:\n",
        "  train_set = pickle.load(f)\n",
        "  test_set = pickle.load(f)\n",
        "  cate_list = pickle.load(f)\n",
        "  user_count, item_count, cate_count = pickle.load(f)\n",
        "\n",
        "\n",
        "gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "with tf.Session(\n",
        "    config=tf.ConfigProto(gpu_options=gpu_options)\n",
        "    ) as sess:\n",
        "\n",
        "  model = Model(user_count, item_count, cate_count, cate_list)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(tf.local_variables_initializer())\n",
        "\n",
        "  best_auc = 0.0\n",
        "  lr = 1.0\n",
        "  for epoch in range(50):\n",
        "\n",
        "    if epoch % 100 == 0 and epoch != 0:\n",
        "      lr *= 0.5\n",
        "\n",
        "    epoch_size = train_set.shape[0] // train_batch_size\n",
        "    loss_sum = 0.0\n",
        "    for _, uij in DataInput(train_set, train_batch_size):\n",
        "      loss = model.train(sess, uij, lr)\n",
        "      loss_sum += loss\n",
        "\n",
        "    epoch += 1\n",
        "    print('epoch: %d\\ttrain_loss: %.2f\\tlr: %.2f' %\n",
        "          (epoch, loss_sum / epoch_size, lr), end='\\t')\n",
        "\n",
        "    test_auc = model.eval(sess, test_set)\n",
        "    print('test_auc: %.4f' % test_auc, flush=True)\n",
        "\n",
        "    if best_auc < test_auc:\n",
        "      best_auc = test_auc\n",
        "      model.save(sess, 'save_path/ckpt')\n",
        "\n",
        "  print('best test_auc:', best_auc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}